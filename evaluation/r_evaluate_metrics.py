import os
import json
import argparse
import random
import math
from collections import defaultdict

import tiktoken

from metrics import (
    qa_f1_score,
    rouge_score,
    classification_score,
    retrieval_score,
    count_score,
    code_sim_score
)

def parse_args():
    p = argparse.ArgumentParser(description="Evaluate metrics on the output of fetch_responses.py and count input tokens")
    p.add_argument(
        '--input_json', required=True,
        help="Original samples JSON (including context and compressed_context)"
    )
    p.add_argument(
        '--answers_json', required=True,
        help="Path to the answers JSON file generated by fetch_responses.py"
    )
    p.add_argument(
        '--sample_size', type=float, default=1.0,
        help="Sampling proportion for each subset, between (0,1]; 1 means all"
    )
    p.add_argument(
        '--seed', type=int, default=42,
        help="Random seed for reproducible sampling"
    )
    p.add_argument(
        '--output_metrics', required=True,
        help="Path to output JSON file for evaluation results"
    )
    p.add_argument(
        '--target', choices=['llm_answers', 'llm_rs_answers'],
        default='llm_answers',
        help="Target to evaluate: llm_answers or llm_rs_answers"
    )
    p.add_argument(
        '--use_original', action='store_true',
        help="Use original context when counting tokens (defaults to compressed_context)"
    )
    return p.parse_args()

# ----------------------------------------
# Mapping from each dataset to its metric function
dataset2metric = {
    "narrativeqa":           qa_f1_score,
    "qasper":                qa_f1_score,
    "multifieldqa_en":       qa_f1_score,
    "hotpotqa":              qa_f1_score,
    "2wikimqa":              qa_f1_score,
    "musique":               qa_f1_score,
    "gov_report":            rouge_score,
    "qmsum":                 rouge_score,
    "multi_news":            rouge_score,
    "trec":                  classification_score,
    "triviaqa":              qa_f1_score,
    "samsum":                rouge_score,
    "passage_retrieval_en":  retrieval_score,
    "passage_count":         count_score,
    "lcc":                   code_sim_score,
    "repobench-p":           code_sim_score,
}

task_groups = {
    "SingleDoc": ["narrativeqa", "qasper", "multifieldqa_en"],
    "MultiDoc":  ["hotpotqa", "2wikimqa", "musique"],
    "Summ.":     ["gov_report", "qmsum", "multi_news"],
    "FewShot":   ["trec", "triviaqa", "samsum"],
    "Synth.":    ["passage_count", "passage_retrieval_en"],
    "Code":      ["lcc", "repobench-p"],
}
# ----------------------------------------

def main():
    args = parse_args()
    random.seed(args.seed)

    orig = json.load(open(args.input_json, 'r', encoding='utf-8'))
    orig_map = {rec['_id']: rec for rec in orig}

    records = json.load(open(args.answers_json, 'r', encoding='utf-8'))

    if args.sample_size < 1.0:
        grouped = defaultdict(list)
        for rec in records:
            grouped[rec['dataset']].append(rec)
        sampled = []
        for ds, recs in grouped.items():
            k = min(math.ceil(args.sample_size * len(recs)), len(recs))
            sampled.extend(random.sample(recs, k))
        records = sampled
        print(f"[Info] Sample rate {args.sample_size:.2f}, total {len(records)} records")
    else:
        print(f"[Info] Using all {len(records)} records")

    encoding = tiktoken.encoding_for_model("gpt-4")
    token_sums = defaultdict(int)
    token_counts = defaultdict(int)

    scores = defaultdict(list)
    for rec in records:
        ds = rec['dataset']
        o = orig_map.get(rec['_id'], {})
        ctx_field = 'context' if args.use_original else 'compressed_context'
        ctx = o.get(ctx_field, "")
        n_tok = len(encoding.encode(ctx))
        token_sums[ds] += n_tok
        token_counts[ds] += 1

        pred_list = rec.get(args.target, [])
        pred = pred_list[0] if isinstance(pred_list, list) and pred_list else pred_list or ""
        
        metric_fn = dataset2metric[ds]
        kwargs = {}
        if metric_fn is classification_score:
            kwargs['all_classes'] = rec.get('all_classes', [])
        gt_list = rec.get('answers', [])
        
        if gt_list:
            score = max(metric_fn(pred, gt, **kwargs) for gt in gt_list)
        else:
            score = 0.0
        scores[ds].append(score)

    per_dataset_score = {
        ds: (sum(vs) / len(vs)) if vs else 0.0
        for ds, vs in scores.items()
    }
    per_dataset_avg_tokens = {
        ds: (token_sums[ds] / token_counts[ds]) if token_counts[ds] else 0.0
        for ds in token_sums
    }
    overall_avg_tokens = (
        sum(token_sums.values()) / sum(token_counts.values())
        if token_counts else 0.0
    )

    summary = {}
    for group, ds_list in task_groups.items():
        vals = [per_dataset_score.get(ds, 0.0) for ds in ds_list]
        summary[group] = (sum(vals) / len(vals)) if vals else 0.0
    summary['AVG'] = (
        sum(per_dataset_score.values()) / len(per_dataset_score)
        if per_dataset_score else 0.0
    )

    print(f"\n=== Per-dataset scores & avg tokens ({args.output_metrics}) ===")
    for ds in sorted(per_dataset_score):
        print(f"{ds:20s}: score {per_dataset_score[ds]:.4f} | avg tokens {per_dataset_avg_tokens[ds]:.1f}")

    print(f"\n=== Group summary & overall avg tokens ({args.output_metrics}) ===")
    for grp in task_groups:
        print(f"{grp:10s}: {summary[grp]:.4f}")
    print(f"{'AVG':10s}: {summary['AVG']:.4f}")
    print(f"Overall avg tokens: {overall_avg_tokens:.1f}")

    metrics_output = {
        "per_dataset_score":      per_dataset_score,
        "per_dataset_avg_tokens": per_dataset_avg_tokens,
        "summary_score":          summary,
        "overall_avg_tokens":     overall_avg_tokens
    }
    save_dir = os.path.dirname(args.output_metrics)
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
    
    with open(args.output_metrics, 'w', encoding='utf-8') as f:
        json.dump(metrics_output, f, ensure_ascii=False, indent=2)


    print(f"\nâœ… Evaluation completed, results written to {args.output_metrics}")

if __name__ == "__main__":
    main()
